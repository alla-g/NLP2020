{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Елизавета Клыкова, БКЛ181\n",
    "# Домашнее задание № 2. Сравнение морфологических тэггеров\n",
    "\n",
    "1. Написать русский и английский тексты (100+ слов), содержащие какие-либо трудные или неоднозначные для POS-теггинга моменты. Разметить их (только часть речи!). Объяснить, какие моменты являются трудными/неоднозначными и почему. (2 балла)\n",
    "2. Проанализировать тексты с помощью трех POS-теггеров для каждого языка: pymorphy2, mystem, Natasha для русского; SpaCy, Flair, NLTK для английского. (4 балла)\n",
    "3. Оценить accurary для каждого теггера. Для этого сначала нужно свести разметку к единому стандарту с помощью отдельной функции, а затем с помощью другой функции сравнить полученные результаты с размеченным вручную эталоном. (2 балла)\n",
    "4. Выбрать лучший теггер для русского языка и написать функцию, повышающую качество работы программы из д/з 1. Выделить 3 вида синтаксических групп (не + какая-либо часть речи или N/NP + наречие и т.д.), использование которых могло бы улучшить работу программы. Написать функцию, выделяющую такие группы в тексте, используя что-либо из списка: chunking и regexp grammar, Natasha syntax parser, код с последнего занятия по SpaCy. (2 балла за функцию + 1 балл за пояснение, почему нужны именно такие группы).\n",
    "5. Бонус. Встроить эту функцию в программу для д/з 1 и сравнить качество работы программы с ней и без нее (2 балла).\n",
    "\n",
    "**Примечание для проверяющего:**\n",
    "1) Файлы rus_text и eng_text содержат исходный текст. Для удобства он так же выводится в программе.\n",
    "2) Файлы rus_tokens и eng_tokens содержат словоформы, записанные каждая с новой строки. Эти файлы создаются программой и дальше никак не используются.\n",
    "3) Файлы rus_tokens2 и eng_tokens2 - копии файлов из предыдущего файла, в которые фручную внесена разметка частеречная разметка. (Я создала копии, т.к. люблю перезапустить программу и потерять все изменения в созданных файлах))))\n",
    "4) Файлы rus_ann и eng_ann содержат полные таблицы с разметкой всеми нужными парсерами. Туда также входят леммы (для каждого парсера свои, для ручной разметки лемм нет). Можно заглянуть в эти файлы, чтобы увидеть, что выделенные мною проблемные места действительно оказались проблемными.\n",
    "\n",
    "## Пункт 1: подготовка текстов\n",
    "\n",
    "Перед началом работы импортируем все необходимые модули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import navec\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import spacy\n",
    "import string\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "from ipymarkup import show_span_ascii_markup as show_markup\n",
    "from navec import Navec\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.metrics import accuracy_score\n",
    "from slovnet import NER\n",
    "from string import punctuation\n",
    "from tqdm.auto import tqdm\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Русский и английский тексты были записаны в файлы rus_text.txt и eng_text.txt соответственно. Для удобства разметки считаем тексты, очистим их он пунктуации и разделим на слова, а затем запишем в новый файл (каждое слово на новой строке, после слова табуляция - нужно, чтобы потом этот файл можно было вручную доразметить и загрузить в pandas dataframe).\n",
    "\n",
    "В качестве основы для разметки была взята разметка, принятая в НКРЯ (https://ruscorpora.ru/new/corpora-morph.html).\n",
    "\n",
    "Поскольку разным парсерам может потребоваться разный формат данных, считаем тексты из файлов и запишем каждый в три разных переменных: text (сырой текст), word_list (список словоформ, очищенных от всей пунктуации, кроме дефиса и апострофа) и token_list (список всех токенов текста, включая пунктуацию)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(filename, new_file):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    token_list = word_tokenize(text)\n",
    "    punct = list(re.sub(\"[-']\", '', string.punctuation))\n",
    "    other_punct = ['``', '\\\"\\\"', '...', '--', '–', '—',\n",
    "                   '«', '»', '“', '”', '’', '***', '…']\n",
    "    all_punct = punct + other_punct\n",
    "    rx = '[' + re.escape(''.join(all_punct)) + ']'\n",
    "    word_list = word_tokenize((re.sub(rx, ' ', text)))\n",
    "    token_text = '\\t\\n'.join(word_list)\n",
    "    with open(new_file, 'w', encoding='utf-8') as f1:\n",
    "        f1.write(token_text)\n",
    "    return text, word_list, token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_text, rus_words, rus_tokens = tokenize_text(\n",
    "    'rus_text.txt', 'rus_tokens.txt')\n",
    "eng_text, eng_words, eng_tokens = tokenize_text(\n",
    "    'eng_text.txt', 'eng_tokens.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на тексты с точки зрения неоднозначных для автоматической разметки моментов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Я на мели, - сходу предупреждаю я, снимая пальто и проходя в комнату. Навстречу выходит Алланова собака, ньюфка, которая за полгода догнала и перегнала меня в весе. Теперь на ней можно ездить верхом.\\n- Никогда такого не было и вот опять, - говорит Аллан и закатывает глаза. Все такая же язва. На душе у меня вдруг становится спокойно.\\nСолнце уже село, а единственная в комнате люстра, в свое время купленная в секонде, приказала долго жить еще до прошлой вечеринки. Конечно, времени починить ее за полгода у Аллана не нашлось. Я свечу ему фонариком на телефоне, пока он достает из шкафа старую оплавившуюся свечу и спички.\\n- Скоро всемирный день лени. Я подарю тебе лампу как главному лентяю.\\n- А когда там всемирный день шила в одном месте? Надо не забыть тебя поздравить.\\nМы беззлобно переругиваемся еще с четверть часа, когда звонит Ирэн и просит помочь ей донести сумки с продуктами от метро. Аллан говорит, что выйдет ей навстречу, и добавляет:\\n- Надеюсь, Салли не задушит мою собаку, пока нас нет.\\n- Как знать, - задумчиво говорю я, рассматривая ньюфку. Та, кажется, не против быть задушенной, так что я валю ее на пол, и она пытается лизнуть меня в нос, виляя хвостом и чуть ли не пища от радости.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неоднозначными моментами в тексте являются, например, частичные омонимы (мели, верхом, село, пища и т.д.) и неизменяемые слова, которые могут относиться к разным частям речи (навстречу как предлог и наречие, всё как наречие). Кроме того, присутствуют редкие нерусские имена (Салли) и лексемы, которые не встречаются в литературном русском: ньюфка (собака породы ньюфаундленд), секонд (магазин секонд-хенд)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Finally, drinks are served, and Simon drinks his beer as though it's the most delicious thing he's had in his entire life. We talk about the most random things, from the new lead he has in his case about lead poisoning to that god-awful seminar he is going to lead in two weeks. We end up discussing the works of a modern Nauruan artist whose name I don't even hope to remember, and after another round of drinks (tequila this time) I tell Simon about a guy that works in the office next door and has the most round face imagineable. Around midnight there is no more room for anything liquid in our stomachs and no cash in our wallets, so we decide to head out. Simon's car is parked around the corner and at this point he wants nothing more than to get in it and sleep it off for the next two weeks, but I point towards a cab waiting on the other side of the road, make Simon turn around and practically drag him across the street.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В английском тексте тоже много неоднозначных моментов, которые могут быть размечены неверно, если парсер не учитывает контекст. В числе таких моментов омонимичные слова (drinks, lead - 3 варианта!, round), слова типа around (предлог и наречие) и редкие слова (Nauruan - прилагательное, образованное от имени собственного Nauru - названия карликового государства)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создав копии файлов, внесем в них ручную разметку (я создаю именно копии, чтобы не потерять разметку при возможном перезапуске программы целиком). Добавим названия столбцов в первую строку. Затем прочитаем файлы и преобразуем их в pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_ann = pd.read_csv('rus_tokens2.txt', sep='\\t')\n",
    "eng_ann = pd.read_csv('eng_tokens2.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пункт 2: использование парсеров\n",
    "### Пункт 2.1. Русский текст\n",
    "#### Pymorphy2\n",
    "Начнем с разметки русского текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_parsed = [morph.parse(w)[0] for w in rus_words]\n",
    "pm_lemmas = [lem.normal_form for lem in pm_parsed]\n",
    "pm_POS = [str(lem.tag.POS) for lem in pm_parsed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_POS = [str(lem.tag.POS) for lem in pm_parsed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pm_POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем записывать результат разбора в датафрейм, убедимся, что число словоформ совпадает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "      <th>pm2_lemmas</th>\n",
       "      <th>pm2_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Я</td>\n",
       "      <td>SPRON</td>\n",
       "      <td>я</td>\n",
       "      <td>NPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>на</td>\n",
       "      <td>PREP</td>\n",
       "      <td>на</td>\n",
       "      <td>PREP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>мели</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>мелить</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>сходу</td>\n",
       "      <td>ADV</td>\n",
       "      <td>сходу</td>\n",
       "      <td>ADVB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>предупреждаю</td>\n",
       "      <td>VERB</td>\n",
       "      <td>предупреждать</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>ли</td>\n",
       "      <td>PART</td>\n",
       "      <td>ли</td>\n",
       "      <td>PRCL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>не</td>\n",
       "      <td>PART</td>\n",
       "      <td>не</td>\n",
       "      <td>PRCL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>пища</td>\n",
       "      <td>VERB</td>\n",
       "      <td>пища</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>от</td>\n",
       "      <td>PREP</td>\n",
       "      <td>от</td>\n",
       "      <td>PREP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>радости</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>радость</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word    POS     pm2_lemmas pm2_POS\n",
       "0               Я  SPRON              я    NPRO\n",
       "1              на   PREP             на    PREP\n",
       "2            мели   NOUN         мелить    VERB\n",
       "3           сходу    ADV          сходу    ADVB\n",
       "4    предупреждаю   VERB  предупреждать    VERB\n",
       "..            ...    ...            ...     ...\n",
       "195            ли   PART             ли    PRCL\n",
       "196            не   PART             не    PRCL\n",
       "197          пища   VERB           пища    NOUN\n",
       "198            от   PREP             от    PREP\n",
       "199       радости   NOUN        радость    NOUN\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus_ann['pm2_lemmas'] = pm_lemmas\n",
    "rus_ann['pm2_POS'] = pm_POS\n",
    "rus_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mystem\n",
    "На вход подаем \"сырой\" текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "ms_parsed = m.analyze(rus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_lemmas = []\n",
    "ms_POS = []\n",
    "for token in ms_parsed:\n",
    "    if 'analysis' in token:\n",
    "        lemma = token['analysis'][0]['lex']\n",
    "        pos = token['analysis'][0]['gr'].split('=')[0].split(',')[0]\n",
    "        ms_lemmas.append(lemma)\n",
    "        ms_POS.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ms_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "      <th>pm2_lemmas</th>\n",
       "      <th>pm2_POS</th>\n",
       "      <th>ms_lemmas</th>\n",
       "      <th>ms_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Я</td>\n",
       "      <td>SPRON</td>\n",
       "      <td>я</td>\n",
       "      <td>NPRO</td>\n",
       "      <td>я</td>\n",
       "      <td>SPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>на</td>\n",
       "      <td>PREP</td>\n",
       "      <td>на</td>\n",
       "      <td>PREP</td>\n",
       "      <td>на</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>мели</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>мелить</td>\n",
       "      <td>VERB</td>\n",
       "      <td>мель</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>сходу</td>\n",
       "      <td>ADV</td>\n",
       "      <td>сходу</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>сходу</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>предупреждаю</td>\n",
       "      <td>VERB</td>\n",
       "      <td>предупреждать</td>\n",
       "      <td>VERB</td>\n",
       "      <td>предупреждать</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>ли</td>\n",
       "      <td>PART</td>\n",
       "      <td>ли</td>\n",
       "      <td>PRCL</td>\n",
       "      <td>ли</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>не</td>\n",
       "      <td>PART</td>\n",
       "      <td>не</td>\n",
       "      <td>PRCL</td>\n",
       "      <td>не</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>пища</td>\n",
       "      <td>VERB</td>\n",
       "      <td>пища</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>пища</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>от</td>\n",
       "      <td>PREP</td>\n",
       "      <td>от</td>\n",
       "      <td>PREP</td>\n",
       "      <td>от</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>радости</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>радость</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>радость</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word    POS     pm2_lemmas pm2_POS      ms_lemmas ms_POS\n",
       "0               Я  SPRON              я    NPRO              я   SPRO\n",
       "1              на   PREP             на    PREP             на     PR\n",
       "2            мели   NOUN         мелить    VERB           мель      S\n",
       "3           сходу    ADV          сходу    ADVB          сходу    ADV\n",
       "4    предупреждаю   VERB  предупреждать    VERB  предупреждать      V\n",
       "..            ...    ...            ...     ...            ...    ...\n",
       "195            ли   PART             ли    PRCL             ли   PART\n",
       "196            не   PART             не    PRCL             не   PART\n",
       "197          пища   VERB           пища    NOUN           пища      S\n",
       "198            от   PREP             от    PREP             от     PR\n",
       "199       радости   NOUN        радость    NOUN        радость      S\n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus_ann['ms_lemmas'] = ms_lemmas\n",
    "rus_ann['ms_POS'] = ms_POS\n",
    "rus_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "morph_vocab = MorphVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc(rus_text)\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_parsed = [t for t in doc.tokens if t.pos != 'PUNCT']\n",
    "nt_POS = [t.pos for t in nt_parsed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nt_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in nt_parsed:\n",
    "    token.lemmatize(morph_vocab)\n",
    "nt_lemmas = [t.lemma for t in nt_parsed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nt_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "      <th>pm2_lemmas</th>\n",
       "      <th>pm2_POS</th>\n",
       "      <th>ms_lemmas</th>\n",
       "      <th>ms_POS</th>\n",
       "      <th>nt_lemmas</th>\n",
       "      <th>nt_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Я</td>\n",
       "      <td>SPRON</td>\n",
       "      <td>я</td>\n",
       "      <td>NPRO</td>\n",
       "      <td>я</td>\n",
       "      <td>SPRO</td>\n",
       "      <td>я</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>на</td>\n",
       "      <td>PREP</td>\n",
       "      <td>на</td>\n",
       "      <td>PREP</td>\n",
       "      <td>на</td>\n",
       "      <td>PR</td>\n",
       "      <td>на</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>мели</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>мелить</td>\n",
       "      <td>VERB</td>\n",
       "      <td>мель</td>\n",
       "      <td>S</td>\n",
       "      <td>мель</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>сходу</td>\n",
       "      <td>ADV</td>\n",
       "      <td>сходу</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>сходу</td>\n",
       "      <td>ADV</td>\n",
       "      <td>сход</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>предупреждаю</td>\n",
       "      <td>VERB</td>\n",
       "      <td>предупреждать</td>\n",
       "      <td>VERB</td>\n",
       "      <td>предупреждать</td>\n",
       "      <td>V</td>\n",
       "      <td>предупреждать</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>ли</td>\n",
       "      <td>PART</td>\n",
       "      <td>ли</td>\n",
       "      <td>PRCL</td>\n",
       "      <td>ли</td>\n",
       "      <td>PART</td>\n",
       "      <td>ли</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>не</td>\n",
       "      <td>PART</td>\n",
       "      <td>не</td>\n",
       "      <td>PRCL</td>\n",
       "      <td>не</td>\n",
       "      <td>PART</td>\n",
       "      <td>не</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>пища</td>\n",
       "      <td>VERB</td>\n",
       "      <td>пища</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>пища</td>\n",
       "      <td>S</td>\n",
       "      <td>пища</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>от</td>\n",
       "      <td>PREP</td>\n",
       "      <td>от</td>\n",
       "      <td>PREP</td>\n",
       "      <td>от</td>\n",
       "      <td>PR</td>\n",
       "      <td>от</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>радости</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>радость</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>радость</td>\n",
       "      <td>S</td>\n",
       "      <td>радость</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word    POS     pm2_lemmas pm2_POS      ms_lemmas ms_POS  \\\n",
       "0               Я  SPRON              я    NPRO              я   SPRO   \n",
       "1              на   PREP             на    PREP             на     PR   \n",
       "2            мели   NOUN         мелить    VERB           мель      S   \n",
       "3           сходу    ADV          сходу    ADVB          сходу    ADV   \n",
       "4    предупреждаю   VERB  предупреждать    VERB  предупреждать      V   \n",
       "..            ...    ...            ...     ...            ...    ...   \n",
       "195            ли   PART             ли    PRCL             ли   PART   \n",
       "196            не   PART             не    PRCL             не   PART   \n",
       "197          пища   VERB           пища    NOUN           пища      S   \n",
       "198            от   PREP             от    PREP             от     PR   \n",
       "199       радости   NOUN        радость    NOUN        радость      S   \n",
       "\n",
       "         nt_lemmas nt_POS  \n",
       "0                я   PRON  \n",
       "1               на    ADP  \n",
       "2             мель   NOUN  \n",
       "3             сход   NOUN  \n",
       "4    предупреждать   VERB  \n",
       "..             ...    ...  \n",
       "195             ли   PART  \n",
       "196             не   PART  \n",
       "197           пища   NOUN  \n",
       "198             от    ADP  \n",
       "199        радость   NOUN  \n",
       "\n",
       "[200 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus_ann['nt_lemmas'] = nt_lemmas\n",
    "rus_ann['nt_POS'] = nt_POS\n",
    "rus_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 2.2. Английский текст\n",
    "#### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc2 = nlp(eng_text)\n",
    "\n",
    "sp_lemmas = []\n",
    "sp_POS = []\n",
    "for sent in doc2.sents:\n",
    "    lemmas = [t.lemma_ for t in sent if t.pos_ != 'PUNCT']\n",
    "    pos = [t.pos_ for t in sent if t.pos_ != 'PUNCT']\n",
    "    sp_lemmas.extend(lemmas)\n",
    "    sp_POS.extend(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sp_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "      <th>sp_lemmas</th>\n",
       "      <th>sp_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Finally</td>\n",
       "      <td>ADV</td>\n",
       "      <td>finally</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>drinks</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>drink</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>are</td>\n",
       "      <td>AUX</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>served</td>\n",
       "      <td>VERB</td>\n",
       "      <td>serve</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>and</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>drag</td>\n",
       "      <td>VERB</td>\n",
       "      <td>drag</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>him</td>\n",
       "      <td>SPRON</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>across</td>\n",
       "      <td>PREP</td>\n",
       "      <td>across</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>street</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>street</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word    POS sp_lemmas sp_POS\n",
       "0    Finally    ADV   finally    ADV\n",
       "1     drinks   NOUN     drink   NOUN\n",
       "2        are    AUX        be    AUX\n",
       "3     served   VERB     serve   VERB\n",
       "4        and   CONJ       and  CCONJ\n",
       "..       ...    ...       ...    ...\n",
       "180     drag   VERB      drag   VERB\n",
       "181      him  SPRON    -PRON-   PRON\n",
       "182   across   PREP    across    ADP\n",
       "183      the    DET       the    DET\n",
       "184   street   NOUN    street   NOUN\n",
       "\n",
       "[185 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_ann['sp_lemmas'] = sp_lemmas\n",
    "eng_ann['sp_POS'] = sp_POS\n",
    "eng_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flair\n",
    "Поскольку Flair не умеет лемматизировать словоформы, получим только части речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-18 17:25:34,669 loading file C:\\Users\\Елизавета Клыкова\\.flair\\models\\en-pos-ontonotes-v0.5.pt\n"
     ]
    }
   ],
   "source": [
    "tagger = SequenceTagger.load('pos')\n",
    "splitter = SegtokSentenceSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = splitter.split(eng_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_tagged = []\n",
    "for sent in sentences:\n",
    "    tagger.predict(sent)\n",
    "    fl_tagged.append(sent.to_tagged_string())\n",
    "fl_tagged = ' '.join(fl_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_POS = re.findall('<[A-Z|$]+?>', fl_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fl_POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь возникает проблема: в тексте есть слово \"god-awful\", которое SpaCy считает за два слова, а flair - за одно. На самом деле это более верный подход, но в ручной разметке это слово разбито на два, поэтому придется \"испортить\" разметку flair'а, добавив в нее отдельную часть речи для слова \"god\". Чтобы это сделать, получим номер этого слова, обратившись к списку лемм, полученному от SpaCy (работает, т.к. словоформа \"god\" встречается в тексте один раз):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = sp_lemmas.index('god')\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавим в список fl_POS тег NN на место 45-го элемента, а остальные сдвинем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fl_POS = fl_POS[0:45] + ['NN'] + fl_POS[45::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_fl_POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что этот метод далек от оптимального. Я решаю проблему именно так, поскольку нас интересует сравнение разных теггеров и совпадение количества словоформ является ключевым моментом. Кроме того, в нашем тексте такая ситуация встретилась только один раз, поэтому представляется возможным просто обратиться к слову по индексу и добавить нужный тег. В реальной задаче по разметке такой подход невозможен, а главное, не имеет смысла: flair в данном случае прав, и дорабатывать нужно не его разметку, а разметку тех парсеров, которые разбивают такие лексемы на две части."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "      <th>sp_lemmas</th>\n",
       "      <th>sp_POS</th>\n",
       "      <th>fl_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Finally</td>\n",
       "      <td>ADV</td>\n",
       "      <td>finally</td>\n",
       "      <td>ADV</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>drinks</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>drink</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>are</td>\n",
       "      <td>AUX</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>served</td>\n",
       "      <td>VERB</td>\n",
       "      <td>serve</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>and</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>drag</td>\n",
       "      <td>VERB</td>\n",
       "      <td>drag</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>him</td>\n",
       "      <td>SPRON</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>across</td>\n",
       "      <td>PREP</td>\n",
       "      <td>across</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>street</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>street</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word    POS sp_lemmas sp_POS fl_POS\n",
       "0    Finally    ADV   finally    ADV     RB\n",
       "1     drinks   NOUN     drink   NOUN    NNS\n",
       "2        are    AUX        be    AUX    VBP\n",
       "3     served   VERB     serve   VERB    VBN\n",
       "4        and   CONJ       and  CCONJ     CC\n",
       "..       ...    ...       ...    ...    ...\n",
       "180     drag   VERB      drag   VERB     VB\n",
       "181      him  SPRON    -PRON-   PRON    PRP\n",
       "182   across   PREP    across    ADP     IN\n",
       "183      the    DET       the    DET     DT\n",
       "184   street   NOUN    street   NOUN     NN\n",
       "\n",
       "[185 rows x 5 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_ann['fl_POS'] = [re.sub('[<|>]', '', pos) for pos in new_fl_POS]\n",
    "eng_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK\n",
    "Поскольку NLTK не учитывает контекст, подадим на вход просто список словоформ - так будет быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tagged = nltk.pos_tag(eng_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "nltk_POS = []\n",
    "nltk_lemmas = []\n",
    "for word in nltk_tagged:\n",
    "    pos = nltk.tag.mapping.map_tag('en-ptb', 'universal', word[1])\n",
    "    nltk_POS.append(pos)\n",
    "    if pos == 'ADJ':\n",
    "        lem_pos = 'a'\n",
    "    elif pos == 'ADJ_SAT':\n",
    "        lem_pos = 's'\n",
    "    elif pos == 'ADV':\n",
    "        lem_pos = 'r'\n",
    "    elif pos == 'VERB':\n",
    "        lem_pos = 'v'\n",
    "    else:\n",
    "        lem_pos = 'n'  # дефолтный тег\n",
    "    lemma = wnl.lemmatize(word[0], pos=lem_pos)\n",
    "    nltk_lemmas.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "      <th>sp_lemmas</th>\n",
       "      <th>sp_POS</th>\n",
       "      <th>fl_POS</th>\n",
       "      <th>nltk_lemmas</th>\n",
       "      <th>nltk_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Finally</td>\n",
       "      <td>ADV</td>\n",
       "      <td>finally</td>\n",
       "      <td>ADV</td>\n",
       "      <td>RB</td>\n",
       "      <td>Finally</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>drinks</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>drink</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>drink</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>are</td>\n",
       "      <td>AUX</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBP</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>served</td>\n",
       "      <td>VERB</td>\n",
       "      <td>serve</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>serve</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>and</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>and</td>\n",
       "      <td>CONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>drag</td>\n",
       "      <td>VERB</td>\n",
       "      <td>drag</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "      <td>drag</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>him</td>\n",
       "      <td>SPRON</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>him</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>across</td>\n",
       "      <td>PREP</td>\n",
       "      <td>across</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>across</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>street</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>street</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>street</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word    POS sp_lemmas sp_POS fl_POS nltk_lemmas nltk_POS\n",
       "0    Finally    ADV   finally    ADV     RB     Finally      ADV\n",
       "1     drinks   NOUN     drink   NOUN    NNS       drink     NOUN\n",
       "2        are    AUX        be    AUX    VBP          be     VERB\n",
       "3     served   VERB     serve   VERB    VBN       serve     VERB\n",
       "4        and   CONJ       and  CCONJ     CC         and     CONJ\n",
       "..       ...    ...       ...    ...    ...         ...      ...\n",
       "180     drag   VERB      drag   VERB     VB        drag     VERB\n",
       "181      him  SPRON    -PRON-   PRON    PRP         him     PRON\n",
       "182   across   PREP    across    ADP     IN      across      ADP\n",
       "183      the    DET       the    DET     DT         the      DET\n",
       "184   street   NOUN    street   NOUN     NN      street     NOUN\n",
       "\n",
       "[185 rows x 7 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_ann['nltk_lemmas'] = nltk_lemmas\n",
    "eng_ann['nltk_POS'] = nltk_POS\n",
    "eng_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пункт 3: оценка accuracy\n",
    "### Пункт 3.1. Стандартизация разметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_tags(tags, lang):\n",
    "    new_tags = []\n",
    "    nouns = ['NOUN', 'PROPN', 'S', 'NNS', 'NNP', 'NN']\n",
    "    verbs = ['VERB', 'V', 'AUX', 'INFN', 'GRND',\n",
    "             'VBP', 'VBZ', 'VBN', 'VBG', 'VB', 'PRTF']\n",
    "    adverbs = ['ADV', 'ADVB', 'ADVPRO', 'PRED', 'RB', 'RBS']\n",
    "    adjectives = ['ADJ', 'A', 'ADJS', 'ADJF', 'JJ', 'JJR']\n",
    "    conjunctions = ['CONJ', 'SCONJ', 'CCONJ', 'CC']\n",
    "    pronouns = ['PRON', 'SPRON', 'APRON', 'SPRO', 'APRO',\n",
    "                'NPRO', 'PRP$', 'PRP', 'EX', 'RP', 'WP$']\n",
    "    prepositions = ['PREP', 'PR', 'ADP']\n",
    "    particles = ['PART', 'PRCL', 'POSS', 'POS', 'TO', 'PRT']\n",
    "    numerals = ['NUM', 'CD']\n",
    "    determiners = ['DET', 'DT', 'WDT']\n",
    "    other = ['INTR', 'IN']  # IN у Flair - и предлог, и союз\n",
    "    for t in tags:\n",
    "        if t in nouns:\n",
    "            new_tags.append('NOUN')\n",
    "        elif t in verbs:\n",
    "            new_tags.append('VERB')\n",
    "        elif t in adverbs:\n",
    "            new_tags.append('ADV')\n",
    "        elif t in adjectives:\n",
    "            new_tags.append('ADJ')\n",
    "        elif t in conjunctions:\n",
    "            new_tags.append('CONJ')\n",
    "        elif t in pronouns:\n",
    "            new_tags.append('PRON')\n",
    "        elif t in prepositions:\n",
    "            new_tags.append('PREP')\n",
    "        elif t in particles:\n",
    "            new_tags.append('PRCL')\n",
    "        elif t in numerals:\n",
    "            new_tags.append('NUM')\n",
    "        elif t in determiners and lang == 'eng':\n",
    "            new_tags.append('DET')\n",
    "        elif t in other:\n",
    "            new_tags.append('other')\n",
    "        elif t == 'DET' and lang == 'rus':\n",
    "            new_tags.append('PRON')\n",
    "        else:\n",
    "            new_tags.append('unknown')\n",
    "    return new_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим получившуюся функцию к нужным столбцам датафреймов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_ann['POS'] = unify_tags(list(rus_ann['POS']), 'rus')\n",
    "rus_ann['pm2_POS'] = unify_tags(list(rus_ann['pm2_POS']), 'rus')\n",
    "rus_ann['ms_POS'] = unify_tags(list(rus_ann['ms_POS']), 'rus')\n",
    "rus_ann['nt_POS'] = unify_tags(list(rus_ann['nt_POS']), 'rus')\n",
    "eng_ann['POS'] = unify_tags(list(eng_ann['POS']), 'eng')\n",
    "eng_ann['sp_POS'] = unify_tags(list(eng_ann['sp_POS']), 'eng')\n",
    "eng_ann['fl_POS'] = unify_tags(list(eng_ann['fl_POS']), 'eng')\n",
    "eng_ann['nltk_POS'] = unify_tags(list(eng_ann['nltk_POS']), 'eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним датафреймы с аннотацией в отдельные файлы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_ann.to_csv('rus_ann.tsv', sep='\\t')\n",
    "eng_ann.to_csv('eng_ann.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь определим accuracy каждого парсера:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pos_tagger(my_pos, tagger_pos, tagger_name):\n",
    "    my_pos = list(my_pos)\n",
    "    tagger_pos = list(tagger_pos)\n",
    "    ignore = ['other', 'unknown']\n",
    "    real_pos = []\n",
    "    predicted_pos = []\n",
    "    for i in range(len(my_pos)):\n",
    "        if my_pos[i] not in ignore and tagger_pos[i] not in ignore:\n",
    "            real_pos.append(my_pos[i])\n",
    "            predicted_pos.append(tagger_pos[i])\n",
    "    res = tagger_name + ' accuracy: ' + str(\n",
    "        round(accuracy_score(predicted_pos, real_pos), 2))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pymorphy2 accuracy: 0.9'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_tagger(rus_ann['POS'], rus_ann['pm2_POS'], 'Pymorphy2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mystem accuracy: 0.94'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_tagger(rus_ann['POS'], rus_ann['ms_POS'], 'Mystem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natasha accuracy: 0.91'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_tagger(rus_ann['POS'], rus_ann['nt_POS'], 'Natasha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SpaCy accuracy: 0.91'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_tagger(eng_ann['POS'], eng_ann['sp_POS'], 'SpaCy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flair accuracy: 0.91'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_tagger(eng_ann['POS'], eng_ann['fl_POS'], 'Flair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLTK accuracy: 0.89'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_tagger(eng_ann['POS'], eng_ann['nltk_POS'], 'NLTK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пункт 4: доработка программы из д/з 1\n",
    "Вполне ожидаемо, Mystem показал наибольшую точность pos-tagging'a, поэтому в этой части использовать будем именно его.\n",
    "\n",
    "Для начала определим, какие виды синтаксических групп нас интересуют.\n",
    "1. Во-первых, это сочетание \"не + глагол\" (ему соответствуют биграммы \"не нравиться\", \"не впечатлять\" и т.д.). Зачем это нужно? Дело в том, что такие сочетания в подавляющем большинстве встречаются именно в отрицательных отзывах, тогда как просто лексемы \"нравиться\" и \"впечатлять\", скорее всего, есть в обоих типах отзывов, а значит, не попали в готовые тональные словари. А ведь это очень показательные моменты.\n",
    "2. Во-вторых, сочетание \"ничто/ничего + прилагательное\" (\"ничего интересного\", \"ничего оригинального\" и т.д.). Опять же, такие сочетания характерны почти исключительно для плохих отзывов, а значит, могут значительно улучшить качество работы.\n",
    "3. Наконец, сочетание \"однозначно + прилагательное/глагол\" (впечатление \"однозначно положительное\", \"однозначно рекомендую\" и т.д.). Ручной поиск показал, что такие сочетания очень характерны для положительных отзывов, а в отрицательных не встречаются (по крайней мере, в нашей выборке).\n",
    "\n",
    "Считаем отзывы из файлов, созданных в первой домашке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('good_reviews.txt', encoding='utf-8') as f:\n",
    "    good_reviews = f.read().split('***')\n",
    "with open('bad_reviews.txt', encoding='utf-8') as f:\n",
    "    bad_reviews = f.read().split('***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем из каждого списка столько, сколько плохих отзывов у нас имеется:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_num = len(bad_reviews)\n",
    "good_reviews = '\\n'.join(good_reviews[0:rev_num])\n",
    "bad_reviews = '\\n'.join(bad_reviews[0:rev_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше сложная система pos-теггинга: Mystem работает гораздо, гораздо быстрее на целом тексте, чем на отдельных предложениях, но нам обязательно нужно сохранить деление на предложения, чтобы правильно выделять внутри них синтаксические группы. Поэтому поступим так:\n",
    "\n",
    "1. Делим текст на предложения.\n",
    "2. Каждое предложение очищаем от пунктуации и токенизируем.\n",
    "3. Соединяем токены в предложения.\n",
    "4. Соединяем предложения в текст, используя в качестве разделителя спецсимвол - знак доллара. Таким образом в дальнейшем мы сможем однозначно определить границу между предложениями.\n",
    "5. Прогоняем весь текст через Mystem.\n",
    "6. Проходимся по токенам: отбираем только словоформы и получаем их лемму и часть речи, пару \"лемма-тег\" записываем в список tagged_word, а tagged_word - в список one_sent. Если очередной токен - это знак доллара, то предложение закончилось: записываем one_sent в список parsed_sents, обнуляем one_sent и повторяем процесс. На выходе имеем список предложений, каждый из которых состоит из списков пар \"лемма-тег\". *(Да, это ужасно, я знаю.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reviews(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    punct = list(re.sub(\"[-']\", '', string.punctuation))\n",
    "    other_punct = ['``', '\\\"\\\"', '...', '--', '–', '—',\n",
    "                   '«', '»', '“', '”', '’', '***', '…']\n",
    "    all_punct = punct + other_punct\n",
    "    rx = '[' + re.escape(''.join(all_punct)) + ']'\n",
    "    tokenized_sents = []\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize((re.sub(rx, ' ', sent)))\n",
    "        tokenized_sents.append(' '.join(words))\n",
    "    sent_text = '$'.join(tokenized_sents)\n",
    "\n",
    "    parsed = m.analyze(sent_text)\n",
    "    parsed_sents = []\n",
    "    one_sent = []\n",
    "    for token in parsed:\n",
    "        tagged_word = []\n",
    "        if 'analysis' in token or token['text'] == '$':\n",
    "            if token['text'] == '$':\n",
    "                parsed_sents.append(one_sent)\n",
    "                one_sent = []\n",
    "            else:\n",
    "                if token['analysis']:\n",
    "                    lemma = token['analysis'][0]['lex']\n",
    "                    pos = token['analysis'][0]['gr'].split(\n",
    "                        '=')[0].split(',')[0]\n",
    "                    tagged_word.append(lemma)\n",
    "                    tagged_word.append(pos)\n",
    "                    one_sent.append(tagged_word)\n",
    "    return parsed_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sents = parse_reviews(good_reviews)\n",
    "bad_sents = parse_reviews(bad_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я решила использовать код с семинара по SpaCy, т.к. мне кажется нелогичным использовать, например, Natasha Syntax Parser, если части речи я размечаю с помощью Mystem'а."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups(sents):\n",
    "    not_verb = []\n",
    "    nothing_adj = []\n",
    "    def_adj_verb = []\n",
    "    nothing = ['ничто', 'ничего']\n",
    "    definitely = ['однозначно', 'однозначный']\n",
    "    for sent in sents:\n",
    "        length = len(sent)\n",
    "        for i, token in enumerate(sent):\n",
    "            if token[0] == 'не':\n",
    "                if sent[i+1][1] == 'V':\n",
    "                    group = token[0] + ' ' + sent[i+1][0]\n",
    "                    not_verb.append(group)\n",
    "            elif token[0] in nothing:\n",
    "                if i + 1 < length:\n",
    "                    if sent[i+1][1] == 'A':\n",
    "                        group = token[0] + ' ' + sent[i+1][0]\n",
    "                        nothing_adj.append(group)\n",
    "            elif token[0] in definitely:\n",
    "                if i + 1 < length:\n",
    "                    if sent[i+1][1] == 'A' or sent[i+1][1] == 'V':\n",
    "                        group = token[0] + ' ' + sent[i+1][0]\n",
    "                        def_adj_verb.append(group)\n",
    "    # для групп \"не + глагол\" установим порог частотности 2:\n",
    "    not_verb = Counter(not_verb)\n",
    "    not_verb = [k for k in list(not_verb.keys()) if not_verb[k] >= 2]\n",
    "    all_groups = nothing_adj + def_adj_verb + not_verb\n",
    "    return Counter(all_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_groups = get_groups(good_sents)\n",
    "bad_groups = get_groups(bad_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остается создать множества уникальных биграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_bigrams = set(good_groups.keys())\n",
    "bad_bigrams = set(bad_groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_bigrams = good_bigrams.intersection(bad_bigrams)\n",
    "good_bigrams.difference_update(common_bigrams)\n",
    "bad_bigrams.difference_update(common_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'не бывать',\n",
       " 'не доходить',\n",
       " 'не думать',\n",
       " 'не задумываться',\n",
       " 'не казаться',\n",
       " 'не лезть',\n",
       " 'не оказываться',\n",
       " 'не отпускать',\n",
       " 'не отрываться',\n",
       " 'не ошибаться',\n",
       " 'не писать',\n",
       " 'не пожалеть',\n",
       " 'не показываться',\n",
       " 'не покидать',\n",
       " 'не поспорить',\n",
       " 'не происходить',\n",
       " 'не раздражать',\n",
       " 'не разочаровываться',\n",
       " 'не расстраивать',\n",
       " 'не слышать',\n",
       " 'не удаваться',\n",
       " 'не хватать',\n",
       " 'не ходить',\n",
       " 'ничто конкретный',\n",
       " 'ничто лишний',\n",
       " 'ничто общий',\n",
       " 'ничто плохой',\n",
       " 'ничто сверхъестественный',\n",
       " 'однозначно быть',\n",
       " 'однозначно впечатлить',\n",
       " 'однозначно заслуживать',\n",
       " 'однозначно положительный',\n",
       " 'однозначно рекомендовать',\n",
       " 'однозначно согласный'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'не верить',\n",
       " 'не вести',\n",
       " 'не видеть',\n",
       " 'не воспринимать',\n",
       " 'не впечатлять',\n",
       " 'не выдавать',\n",
       " 'не делать',\n",
       " 'не дорастать',\n",
       " 'не ждать',\n",
       " 'не зайти',\n",
       " 'не зацеплять',\n",
       " 'не идти',\n",
       " 'не изменяться',\n",
       " 'не иметься',\n",
       " 'не купить',\n",
       " 'не называть',\n",
       " 'не находить',\n",
       " 'не нести',\n",
       " 'не нравиться',\n",
       " 'не огорчать',\n",
       " 'не одеваться',\n",
       " 'не осиливать',\n",
       " 'не открывать',\n",
       " 'не отлипать',\n",
       " 'не отличаться',\n",
       " 'не переставать',\n",
       " 'не повлечь',\n",
       " 'не подходить',\n",
       " 'не помнить',\n",
       " 'не порекомендовать',\n",
       " 'не портить',\n",
       " 'не потерять',\n",
       " 'не появляться',\n",
       " 'не предполагать',\n",
       " 'не придумывать',\n",
       " 'не производить',\n",
       " 'не проникаться',\n",
       " 'не прочитывать',\n",
       " 'не пугать',\n",
       " 'не раскрывать',\n",
       " 'не рекомендовать',\n",
       " 'не связывать',\n",
       " 'не смочься',\n",
       " 'не собираться',\n",
       " 'не советовать',\n",
       " 'не соглашаться',\n",
       " 'не спасать',\n",
       " 'не способствовать',\n",
       " 'не срастаться',\n",
       " 'не тратить',\n",
       " 'не трогать',\n",
       " 'не уважать',\n",
       " 'не увидеть',\n",
       " 'не цеплять',\n",
       " 'ничто великий',\n",
       " 'ничто вкусный',\n",
       " 'ничто годный',\n",
       " 'ничто интересный',\n",
       " 'ничто оригинальный',\n",
       " 'ничто полезный',\n",
       " 'ничто страшный',\n",
       " 'ничто трогательный',\n",
       " 'ничто хороший'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно добавить получившиеся биграммы к тональным словарям из д/з 1 и для каждого отзыва проверять, есть ли в нем такие биграммы. Делать я этого не буду, потому что у меня горит другой дедлайн :("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
